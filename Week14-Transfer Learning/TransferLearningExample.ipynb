{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_test, y_test)= tf.keras.datasets.cifar100.load_data(label_mode=\"fine\")\n",
    "(ds_train,ds_val, ds_test), ds_info = tfds.load(\n",
    "    'fashion_mnist',\n",
    "    split=['train[:80%]','train[80%:]','test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, 32, 32, 3), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "def normalize_transpose_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.transpose(tf.cast(image, tf.float32) / 255.), label\n",
    "def pad_img(image, label):\n",
    "    \"\"\"turns 28 x 28 image into 32 x 32\"\"\"\n",
    "    padded_im=tf.image.pad_to_bounding_box(image, 4, 4, 32, 32)\n",
    "    tripled_im = tf.image.grayscale_to_rgb(padded_im)\n",
    "    return tripled_im, label\n",
    "def get_label(image,label):\n",
    "    return label\n",
    "def get_image(image,label):\n",
    "    return image\n",
    "# this just normalizes the image\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "if transfer:\n",
    "    ds_train = ds_train.map(pad_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# cache and prefetch save memory \n",
    "ds_train = ds_train.cache()\n",
    "# we shuffle data (reduces effects of order when training model)\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train[:80%]'].num_examples)\n",
    "# batch size tells us how many samples are needed for stochastic gradient descent \n",
    "# (a small number like 32 is empirically found to be better)\n",
    "ds_train = ds_train.batch(32)# batch of 32\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "ds_val = ds_val.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "if transfer:\n",
    "    ds_val = ds_val.map(pad_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.cache()\n",
    "ds_val = ds_val.shuffle(ds_info.splits['train[80%:]'].num_examples)\n",
    "ds_val = ds_val.batch(32)# batch of 32\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "ds_test_label=ds_test.map(\n",
    "    get_label, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test_images=ds_test.map(\n",
    "    get_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(ds_test)\n",
    "#image = ds_test.astype(np.float32)\n",
    "#image = np.expand_dims(np.expand_dims(image, axis=2), axis=0)\n",
    "#image = np.concatenate((image, 0.5*image), 0)\n",
    "#image = np.concatenate((image, 0.5*image), 0)\n",
    "#image3D = np.expand_dims(image, axis=0)\n",
    "#image3D = np.concatenate((image3D, 0.5*image3D), 0)\n",
    "\n",
    "ds_test_trans = ds_test.map(normalize_transpose_img)\n",
    "ds_test_trans_images=ds_test_trans.map(get_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test_trans = ds_test_trans.batch(32)\n",
    "ds_test_trans = ds_test_trans.cache()\n",
    "ds_test_trans = ds_test_trans.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "if transfer:\n",
    "    ds_test = ds_test.map(pad_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(32)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_26 (Functional)        (None, 16, 16, 64)        38720     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 40)        23080     \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 7840)              0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 10)                78410     \n",
      "=================================================================\n",
      "Total params: 140,210\n",
      "Trainable params: 140,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 92s 60ms/step - loss: 0.6641 - sparse_categorical_accuracy: 0.8147 - val_loss: 0.2976 - val_sparse_categorical_accuracy: 0.8938\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 89s 59ms/step - loss: 0.2714 - sparse_categorical_accuracy: 0.9006 - val_loss: 0.2576 - val_sparse_categorical_accuracy: 0.9060\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 90s 60ms/step - loss: 0.2290 - sparse_categorical_accuracy: 0.9190 - val_loss: 0.2345 - val_sparse_categorical_accuracy: 0.9158\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 86s 57ms/step - loss: 0.2016 - sparse_categorical_accuracy: 0.9268 - val_loss: 0.2414 - val_sparse_categorical_accuracy: 0.9106\n"
     ]
    }
   ],
   "source": [
    "# this example model \n",
    "#    - first flattens data into a vector (image is 28 x 28)\n",
    "#    - then creates a dense 128-node layer\n",
    "#    - then creates a dropout \"layer\" (it says how many nodes are dropped out in previous layer)\n",
    "#    - then another 128-node layer\n",
    "#    - and finally 10-node layer as the head. \n",
    "# The max value of the head = the predicted image\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if transfer:\n",
    "    # now let's take advantage of a large model with a well-trained set of weights\n",
    "    # load model\n",
    "    #PTmodel = VGG16(include_top=False, weights='imagenet')\n",
    "    # summarize the model\n",
    "    #PTmodel.summary()\n",
    "    #yhat = PTmodel.predict(image)\n",
    "    inp = (32,32,3)\n",
    "    VGG = VGG16(include_top=False,weights='imagenet',input_shape=inp)#,pooling='avg')\n",
    "    # remove the output layer\n",
    "    VGG = tf.keras.Model(inputs=VGG.inputs, outputs=VGG.layers[-16].output)\n",
    "    #we choose not to train weights of VGG network, \n",
    "    #but if you play around with this model\n",
    "    #you could if you wish\n",
    "    #for layer in VGG.layers:\n",
    "    #    layer.trainable = False \n",
    "       \n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(VGG)\n",
    "    #model.add(tf.keras.layers.Flatten())\n",
    "    #model.add(tf.keras.layers.Dense(500,activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(40, kernel_size=3, activation='relu'))\n",
    "    #model.add(tf.keras.layers.Dropout(.5, input_shape=(64,)))\n",
    "    #model.add(tf.keras.layers.Dense(100,activation='relu'))\n",
    "    #model.add(tf.keras.layers.Dropout(.5, input_shape=(32,)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    #model.add(tf.keras.layers.Dense(40,activation='relu'))\n",
    "    #model.add(tf.keras.layers.Dropout(.5, input_shape=(32,)))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    print(model.summary())\n",
    "else:\n",
    "    model = tf.keras.models.Sequential([])\n",
    "    model.add(tf.keras.layers.Conv2D(100, kernel_size=3, activation='relu', input_shape=(28,28,1)))#64\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(40, kernel_size=3, activation='relu'))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    #model.add(tf.keras.layers.Dropout(.5))\n",
    "    #model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(1, 1), padding='valid'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    #tf.keras.layers.Dense(24,activation='relu'),\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    print(model.summary())\n",
    "# this specifies how we find the best NN\n",
    "# - Optimizer like Adam is found to work well\n",
    "# - Loss is \"sparse categorical cross entropy\" (you can choose whatever loss function on keras improves your model)\n",
    "# - We also record accuracy (\"metric\"), this does not affect training\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "# this is stuff we record\n",
    "# \"early stopping\" tells us when we found the optimum without training more epochs\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "# the training of the model\n",
    "# we use ds_train data, and up to 30 epochs (less when Early Stopping is used)\n",
    "# we also record callbacks, and we determine the optimal model by where validation is smallest\n",
    "history=model.fit(\n",
    "    ds_train,\n",
    "    epochs=30,\n",
    "    callbacks=my_callbacks,\n",
    "    validation_data=ds_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall accuracy\n",
    "# pre-trained model: best = 0.7399\n",
    "# No-pre-training: best = 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "0.6634\n"
     ]
    }
   ],
   "source": [
    "gt_labels=list(ds_test_label)\n",
    "predictions=[np.argmax(im) for im in model.predict(ds_test)]\n",
    "print(len(predictions))\n",
    "print(accuracy_score(predictions,gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "313\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 32768 into shape (32,28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-69ea461c1bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwithin_batch_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtensored\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 32768 into shape (32,28,28)"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "print(len(gt_labels))\n",
    "print(len(ds_test))\n",
    "label_names = ['t-shirt/top','trouser','pullover','dress','coat','sandals','shirt','sneaker','bag','ankle boots']\n",
    "batch_size=32\n",
    "for i in range(300):#len(ds_test)-1):\n",
    "    rand_ind=np.random.randint(len(gt_labels))\n",
    "    gt = gt_labels[rand_ind]\n",
    "    pred = predictions[rand_ind]\n",
    "    if pred != gt:\n",
    "        batch_index = int(rand_ind/batch_size)\n",
    "        within_batch_pos = rand_ind % batch_size\n",
    "        dat=[im[:,:,0] for im in np.array(list(ds_test)[batch_index][0])]\n",
    "    \n",
    "        dat=np.array(dat).reshape(32,28,28)\n",
    "        im = dat[within_batch_pos].reshape(28,28)\n",
    "        tensored=tf.convert_to_tensor(dat.reshape(32,28,28,1))\n",
    "        confidence = [np.max(softmax(v)) for v in model.predict([tensored])]\n",
    "        plt.imshow(im,cmap='binary')\n",
    "        plt.title('Predicted: '+label_names[pred]+' Actual: '+label_names[int(gt)])\n",
    "        plt.axes().axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
